\documentclass{article}
\usepackage{float}
\usepackage[parfill]{parskip}
\hbadness=10000
\begin{document}

\title{Coursework (CM3111)\\Big Data Analytics}
\author{Alistair Quinn}
\maketitle 

\par\noindent\rule{\textwidth}{0.4pt}

\section{Data Exploration}
\subsection{Dataset Choice}
I have chosen a dataset that was used in the CoIL 2000 Challenge, which is called the Caravan Insurance Challenge dataset available here: https://www.kaggle.com/uciml/caravan-insurance-challenge 

The data set is free to use for non-comercial use. Although sourced from the UCL Machine Learning organization's kaggle page, the dataset is owned by Sentient Machine Research. 
\subsection{Technology-Platform}
The dataset is contained in a csv file that is 245KB in size. As the dataset is so small I not need to use Big Data technology such as Hadoop. Instead I will use RStudio on a windows PC. I chose the dataset based on my current ability, and I found the idea of the dataset interesting. 
\subsection{Problem Statement & Data Exploration}
Each row in the table corresponds to a post code.The task with this dataset is to identify potential purchasers of caravan insurance policies. The class label in the dataset is called CARAVAN and has two values, 0 or 1. CARAVARN is 1 when that row would potentially purchase a caravan insurance policy. 

First I'll load the dataset I'm using.
<<echo=TRUE,eval=TRUE>>=
#Set WD
setwd("D:/RGU/3rdYear/Semester1/Big Data Analytics/Coursework/wd")
#Load Data
df <- read.csv("Data/caravan-insurance-challenge.csv")
@
I will now explore my dataset to identify its features and learn more about it. 

During my data exploration, I will be visualising my data using a package called ggplot2. This is a plotting system for R. I have chosen to use this package as I have experience with it from the labs in the course. It has some powerful plotting functions, and works in an intuative way. 
<<echo=TRUE,eval=TRUE>>=
library(ggplot2)
@

Going to take a look at the number of rows and columns
<<echo=TRUE,eval=TRUE>>=
#Rows and Cols
nrow(df)
ncol(df)
@
There are currently 9822 rows, and 87 columns in the dataset 

Going to take a look at the features of the dataset
<<echo=TRUE,eval=TRUE>>=
#Names of columns 
names(df)
@
Variables beginning with M are demographic statistics of the postal code. 
\begin{figure}[H]
\includegraphics{images/mainfields.png}}
\end{figure}
Variables beginning with P and A refer to product ownership and insurance statistics of the postal code. Variables beginning with P refer to contribution policies. 
\begin{figure}[H]
\includegraphics{images/contributionfields.png}
\end{figure}
variables beggining with A refer to number of policies.
\begin{figure}[H]
\includegraphics{images/numberoffields.png}
\end{figure}
Going to check the factors of the dataset
<<echo=TRUE,eval=FALSE>>=
#Names of columns 
sapply(df,levels)
@
I have ommited the result of the above code, as it was far too large. Most of the columns in the current data are numeric values but are actually supposed to be factors. I will refactor these columns during pre-processing. The only variable that has been turned into a factor by R is the first one, ORIGIN. I will explore this factor later. 

There are 4 keys that relate to this datset. A key for customer subtype:
\begin{figure}[H]
\includegraphics{images/customersubtype.png}
\end{figure}
A key for average age:
\begin{figure}[H]
\includegraphics{images/averageage.png}
\end{figure}
A key of customer main types:
\begin{figure}[H]
\includegraphics{images/customermaintype.png}
\end{figure}
A key of percentage ranges:
\begin{figure}[H]
\includegraphics{images/percentage.png}
\end{figure}
and a key of total number ranges:
\begin{figure}[H]
\includegraphics{images/totalnumber.png}
\end{figure}
I will use these keys to turn the appropriate columns into factors later, by maping the numeric values in the dataset to the appropritae value from the keys. 

I will now take a look at the class label distribution
<<echo=TRUE,eval=TRUE>>=
#Class label freq
classLabelFreq <- data.frame(df$CARAVAN)
classLabelFreq$df.CARAVAN <- as.factor(df$CARAVAN)

#Class label Distribution Plot 
ggplot(classLabelFreq,aes(x=df.CARAVAN)) + geom_bar() + labs(x="CARAVAN")

#Size of each factor level 
length(classLabelFreq[classLabelFreq$df.CARAVAN=="0",])
length(classLabelFreq[classLabelFreq$df.CARAVAN=="1",])
@

There are 586 records that are likely to want caravan insurance. 9236 records that do not. Dataset has an imbalanced distribution in the class label. I will use a resampling technique during pre-processing to compensate for this. 

Lets take a look at the disribution of Main Customer Type
<<echo=TRUE,eval=TRUE>>=
#Cust main type
custMainType <- data.frame(df$MOSHOOFD,df$CARAVAN)
custMainType$df.MOSHOOFD <- as.factor(custMainType$df.MOSHOOFD)
custMainType$df.CARAVAN <- as.factor(custMainType$df.CARAVAN)

#Plot of Customer Main Type
plot<-ggplot(custMainType,aes(x=reorder(df.MOSHOOFD,df.MOSHOOFD,function(x)-length(x)),fill=df.CARAVAN))
plot<-plot + geom_bar() 
plot<-plot + labs(x="Customer Main Type")
plot
@

Most frequent Main Customer Type is 8:Family with Grown Ups, 2nd Most frequent is 3:Average Family and the 3rd most frequent is 9:conservative families. 

The least frequent type is 4:Career Loners ,2nd least frequent is 6:Cruising Seniors and the 3rd least frequent is 10:Farmers. 

Comparing to where CARAVAN is TRUE, you can see that the two most frequent main types are the same as the whole dataset. Customer Main Type 2: Driven Growers seems to be a bit more prominent where CARAVAN is true. You can also see that there are no instances of group 4: Career Loners in the rows where CARAVAN is true. 

Lets take a closer look at the rows where CARAVAN is TRUE:
<<echo=TRUE,eval=TRUE>>=
#Wants caravan 
wantsCaravan <- df[df$CARAVAN==1,]
wantsCaravan$MOSHOOFD <- as.factor(wantsCaravan$MOSHOOFD)
wantsCaravan$MOSTYPE <- as.factor(wantsCaravan$MOSTYPE)

#Plot of Customer Main Type where wants caravan
plot<-ggplot(wantsCaravan,aes(x=reorder(MOSHOOFD,MOSHOOFD,function(x)-length(x))))
plot<-plot + geom_bar()
plot<-plot + labs(x="Customer Main Type")
plot

#Max and Min
mainCustType = table(wantsCaravan$MOSHOOFD)
names(which.max(mainCustType))
names(which.min(mainCustType))
@

The top 3 Main Customer Types are 8:Family with Grown Ups, 3: Average Family and 2:Driven Growers. As stated before the first 2 types are the same as the dataset as a whole. Type 2:Driven Growers has now overtaken 9:conservative families. 9:Conservative Families is joint 4th. Interesting that category 1: Successful Hedonists has the same number of occurances as 9:Conservaite Families. Hedonists are people who devote their lives to the pursuit of pleasure. There is perhaps a connection there between the idea of traveling by caravan and being a hedonist. 

The 3 least frequent Main Custom Types 6:Cruising Seniors ,10:Farmers and 5:Living Well. As stated before there are no instances of 4:Career Loners when CARAVAN is TRUE. There is too small a number of instances of 4:Career Loners in the whole dataset to really say there is correlation but it is possible. 

Now going to take a look at Customer Subtype
<<echo=TRUE,eval=TRUE>>=
#Sub cust type
subCustType <- data.frame(df$MOSTYPE,df$CARAVAN)
subCustType$df.MOSTYPE <- as.factor(subCustType$df.MOSTYPE)
subCustType$df.CARAVAN <- as.factor(subCustType$df.CARAVAN)

#Plot of Customer subtype
plot<-ggplot(subCustType,aes(x=reorder(df.MOSTYPE,df.MOSTYPE,function(x)-length(x)),fill=df.CARAVAN))
plot<-plot + geom_bar() 
plot<-plot + labs(x="Customer Subtype")
plot
@
The top 3 subtypes are 33:Lower class large families ,38:Traditional families  and 8:Middle class families. 

The bottom 3 are 19:Suburban Youth ,15:Senior cosmopolitans and 17:Fresh masters in the city. The dataset contains mostly data about families. 

Taking a look at when CARAVAN is TRUE, you can see that It doesn't follow the same trend in frequency as the dataset as a whole This might mean that It could be used as a useful predictor. You can also see that there are no rows where CARAVAN is true that have subtypes of 40:Large family farms, 21:Young urban have-nots, 18:Single youth, 16:Students in apartments, 17:Fresh masters in the city, 15:Senior cosmopolitans and 19:Suburban youth. 

This could suggest that postcode areas most comprised with familes are more likely to purchase caravan insurance. 

Going to take a look at customer subtype when CARAVAN is TRUE
<<echo=TRUE,eval=TRUE>>=
#Plot of Customer Subtype where wants caravan 
plot<-ggplot(wantsCaravan,aes(x=reorder(MOSTYPE,MOSTYPE,function(x)-length(x)))) 
plot<-plot + geom_bar() 
plot<-plot + labs(x="Customer Subtype")
plot

#Max and Min 
subCustType = table(wantsCaravan$MOSTYPE)
names(which.max(subCustType))
names(which.min(subCustType))
@
Top 3 subtypes are the same as the datset as a whole but subtype 8:Middle class families is now more frequent than 38:Traditional families. This might mean that areas consisting of lower to middle class families are more likely to purchase caravan insurance than areas with mostly traditional families. 

The bottom 3 subtypes are 27:Seniors in apartments ,26:Own home elderly and 20:Ethnically diverse. This supports the theory that areas with higher amounts of families are the most likely to purchase caravan insurance. 

I am not going to take a look at two columns. MAANTHUI(Number of houses) and MGEMOMV(Avg size of household). MAATHUI is in the range of 1-10 and MGEMOMV is in the range of 1-6. They are the only two numeric values in the dataset the rest are factors. I will look at them together to see if there is any corelation. I will use ggplot2 again to make a scatter plot. These are integer values there will be overlap. 

<<echo=TRUE,eval=TRUE>>=
#Number of Houses and Avg size of household
houseData<-data.frame(df$MAANTHUI,df$MGEMOMV,df$CARAVAN)
houseData$df.CARAVAN<-as.factor(houseData$df.CARAVAN)

#ScatterPlot of both
plot<-ggplot(houseData,aes(x=df.MAANTHUI,y=df.MGEMOMV)) 
plot<-plot + geom_point(aes(shape=df.CARAVAN))
plot<-plot + scale_shape(solid=FALSE)
plot<-plot + labs(x="Number of Houses", y="Average Household Size")
plot
@
Taking a look at the results, average size of household decreases as the number of houses increases which makes sense. Looking at the points where CARAVAN is TRUE, There are no points when the number of houses is greater than 3. There are also no points when the average house size is greater than 5. This shows a potential connection between number of houses and CARAVAN. If I had to remove one of the two variables I would remove Average house size as I think number of houses has a greater corelation to CARAVAN equaling TRUE

I will now take a look at the average age variable, MGEMLEEF. 
<<echo=TRUE,eval=TRUE>>=
#Average Age 
averageAge <- data.frame(df$MGEMLEEF,df$CARAVAN)
averageAge$df.MGEMLEEF <- as.factor(averageAge$df.MGEMLEEF)
averageAge$df.CARAVAN <- as.factor(averageAge$df.CARAVAN)

#Plot of Average Age 
plot<-ggplot(averageAge,aes(x=df.MGEMLEEF,fill=df.CARAVAN))
plot<-plot + geom_bar() 
plot<-plot + labs(x="Average Age")
plot
@
Average age is a factor, where each level is a range of ages. Lowest age range is 1:20-30 years, highest is 6:70-80 years. Looking at the graph, levels 3:40-50, 2:30-40 and 4:50-60 are the top 3 most frequent values. The extremes of 1 and 6 are the two lowest and do not have very many occurances. Looking at instances where CARAVAN is TRUE, They are in the same order as the whole dataset. Except there are no instances where CARAVAN is TRUE that contain 1 and 6 for average age.There might be a trend that areas were the average age is 1:20-30 or 6:70-80 would not buy caravan insurance. There isn't enough data to be sure and as the data where CARAVAN is true follows a similar trend to the data as a whole its unlikely there is a corelation between average age and CARAVAN. 

The first column in the data set is ORIGIN. It has two values
<<echo=TRUE,eval=TRUE>>=
#Levels of ORIGIN
levels(df$ORIGIN)
@
This is the original source of the row from the challenge. The rows are already split into a train set and test set. I will remove this column later during pre-processing, as I plan to resample the data and split the data into train and test sets myself. 

I have noW looked at all the main variables. 

I am not going to take a look at the APERSAUT column, which is the Number of Car Policies column. This is a factor, where each level equates to a range of values. The ranges are defined in the total number key. I have a feeling that it might be an important predictor, so want to try and confirm my theory.  
<<echo=TRUE,eval=TRUE>>=
#Number of Car Policies
numberOfCarPolicies <- data.frame(df$APERSAUT,df$CARAVAN)
numberOfCarPolicies$df.APERSAUT <- as.factor(numberOfCarPolicies$df.APERSAUT)
numberOfCarPolicies$df.CARAVAN <- as.factor(numberOfCarPolicies$df.CARAVAN)

#Plot of APERSAUT
plot<-ggplot(numberOfCarPolicies,aes(x=reorder(df.APERSAUT,df.APERSAUT,function(x)-length(x)),fill=df.CARAVAN)) 
plot<-plot + geom_bar()
plot<-plot + labs(x="Number of Car Policies")
plot
@
There is a factor level of 12 here on the graph. There is only supposed to be levels 0-9. This is likely a mistake in the dataset. When I factor this column later during pre processing and remove n/a values this wrong value should get removed. 

Looking at when CARAVAN is TRUE, there is a possible relation between when the number of car policies are the level 1:1-49 and when CARAVAN is TRUE. 
<<echo=TRUE,eval=TRUE>>=
#Number of Car Policies When Caravan is TRUE
wantsCaravan$APERSAUT <- as.factor(wantsCaravan$APERSAUT)

#Plot of number of car policies (caravan is TRUE)
plot<-ggplot(wantsCaravan,aes(x=APERSAUT))
plot<-plot + geom_bar()
plot<-plot + labs(x="Number of Car Policies")
plot
@
This factor is supposed to have 9 levels in total, but this column factored only contains the lowest 4 possible factors, 0,1,2 and 3. 1 is the most frequent. In this case 1 represents the range of values 1-49. 0 represents 0, 2 represents the range of values 50-99 and 3 represents the range of values 100-199. 

This would suggest that post codes that contain a number of cars in the range of 1-49 are most likey to purchase caravan insurance. There are only 2 rows in the dataset that contain 3 for this column so this could be considered an outlier. Based on the graph the range of values for number of cars could be 0-99, or 0-199 if you include the two rows that have the value 3. There isn't enough data here to be sure. 

I was originally confused by this result. I had assumed that I would find the opposite, that areas with large numbers of cars would likey need caravan insurance as I assumed you would need a car to tow a caravan. After further study of the information about the dataset, in this case CARAVAN actually refers to mobile homes. This would suggest the data is based on american post codes (or area codes) and that 'caravan' refers to a self driving vehicle that you can sleep in rather than a traditional british caravan. 

Based on the graph its possible there is a correlation between this variable and CARAVAN being TRUE. This variable might be an important predictor. 

\subsection{Pre-Proccessing}
I will now pre-process my data before I begin building models

First I will rename all the columns. This will make them a little easier to understand, and means I do not have to keep refering to the keys. I will use the dplyr package to do this.
<<echo=TRUE,eval=TRUE>>=
library(dplyr)
@
This is a package used for manipulating frame objects in r. I will now use the rename function to rename the columns
<<echo=TRUE,eval=TRUE>>=
#Rename columns 
df<-rename(df,Customer_Subtype=MOSTYPE,
           Number_of_Houses=MAANTHUI,
           Avg_Size_Household=MGEMOMV,
           Avg_Age=MGEMLEEF,
           Customer_Main_Type=MOSHOOFD,
           Percentage_Roman_Catholic=MGODRK,
           Percentage_Protestant=MGODPR,
           Percentage_Other_Religion=MGODOV,
           Percentage_No_Religion=MGODGE,
           Percentage_Married=MRELGE,
           Percentage_Living_Together=MRELSA,
           Percentage_Other_Relation=MRELOV,
           Percentage_Singles=MFALLEEN,
           Percentage_Household_without_Children=MFGEKIND,
           Percentage_Household_with_Children=MFWEKIND,
           Percentage_High_Level_Education=MOPLHOOG,
           Percentage_Middle_Level_Education=MOPLMIDD,
           Percentage_Low_Level_Education=MOPLLAAG,
           Percentage_High_Status=MBERHOOG,
           Percentage_Entrepreneur=MBERZELF,
           Percentage_Farmer=MBERBOER,
           Percentage_Middle_Management=MBERMIDD,
           Percentage_Skilled_Labourers=MBERARBG,
           Percentage_Unskilled_Labourers=MBERARBO,
           Percentage_Social_Class_A=MSKA,
           Percentage_Social_Class_B1=MSKB1,
           Percentage_Social_Class_B2=MSKB2,
           Percentage_Social_Class_C=MSKC,
           Percentage_Social_Class_D=MSKD,
           Percentage_Rented_House=MHHUUR,
           Percentage_Home_Owners=MHKOOP,
           Percentage_1_Car=MAUT1,
           Percentage_2_Cars=MAUT2,
           Percentage_No_Car=MAUT0,
           Percentage_National_Health_Service=MZFONDS,
           Private_Health_Insurance=MZPART,
           Percentage_Income_Less_Than_30k=MINKM30,
           Percentage_Income_30_to_40k=MINK3045,
           Percentage_Income_45_to_75k=MINK4575,
           Percentage_Income_75_to_122k=MINK7512,
           Percentage_Income_123k=MINK123M,
           Percentage_Average_Income=MINKGEM,
           Percentage_Purchasing_Power_Class=MKOOPKLA,
           Number_of_Contribution_Private_Third_Party_Insurance=PWAPART,
           Number_of_Contribution_Third_Party_Insurance_firms=PWABEDR,
           Number_of_Contribution_Third_Party_Insurance_agriculture=PWALAND,
           Number_of_Contribution_Car_Policies=PPERSAUT,
           Number_of_Contribution_Delivery_Van_Policies=PBESAUT,
           Number_of_Contribution_Motorcycle_Scooter_Policies=PMOTSCO,
           Number_of_Contribution_Lorry_Policies=PVRAAUT,
           Number_of_Contribution_Trailer_Policies=PAANHANG,
           Number_of_Contribution_Tractor_Policies=PTRACTOR,
           Number_of_Contribution_Agricultural_Machines_Policies=PWERKT,
           Number_of_Contribution_Moped_Policies=PBROM,
           Number_of_Contribution_Life_Insurances=PLEVEN,
           Number_of_Contribution_Private_Accident_Insurance_Policies=PPERSONG,
           Number_of_Contribution_Family_Accidents_Insurance_Policies=PGEZONG,
           Number_of_Contribution_Disability_Insurance_Policies=PWAOREG,
           Number_of_Contribution_Fire_Policies=PBRAND,
           Number_of_Contribution_Surfboard_Policies=PZEILPL,
           Number_of_Contribution_Boat_Policies=PPLEZIER,
           Number_of_Contribution_Bicycle_Policies=PFIETS,
           Number_of_Contribution_Property_Insurance_Policies=PINBOED,
           Number_of_Contribution_Social_Security_Insurance_Policies=PBYSTAND,
           Number_of_Private_Third_Party_Insrance=AWAPART,
           Number_of_Third_Party_Insurance_firms=AWABEDR,
           Number_of_Third_Party_Insrance_agriculture=AWALAND,
           Number_of_Car_Policies=APERSAUT,
           Number_of_Delivery_Van_Policies=ABESAUT,
           Number_of_Motorcycle_Scooter_Policies=AMOTSCO,
           Number_of_Lorry_Policies=AVRAAUT,
           Number_of_Trailer_Policies=AAANHANG,
           Number_of_Tractor_Policies=ATRACTOR,
           Number_of_Agricultural_Machines_Policies=AWERKT,
           Number_of_Moped_Policies=ABROM,
           Number_of_Life_Insurances=ALEVEN,
           Number_of_Private_Accident_Insurance_Policies=APERSONG,
           Number_of_Family_Accidents_Insurance_Policies=AGEZONG,
           Number_of_Disability_Insurance_Policies=AWAOREG,
           Number_of_Fire_Policies=ABRAND,
           Number_of_Surfboard_Policies=AZEILPL,
           Number_of_Boat_Policies=APLEZIER,
           Number_of_Bicycle_Policies=AFIETS,
           Number_of_Property_Insurance_Policies=AINBOED,
           Number_of_Social_Security_Insurance_Policies=ABYSTAND)
@
I will now refactor all the appropriate columns
<<echo=TRUE,eval=TRUE>>==
#Refactoring
#Customer Subtype Refactor
df$Customer_Subtype <- factor(df$Customer_Subtype,
                              levels=c(1:41),
                              labels=c("High Income, expensive child",
                                       "Very Important Provincials",
                                       "High status seniors",
                                       "Affluent senior apartments",
                                       "Mixed seniors",
                                       "Career and childcare",
                                       "Dinki's (Double income no kids)",
                                       "Middle class families",
                                       "Modern, complete families",
                                       "Stable family","Family starters",
                                       "Affluent young families",
                                       "Young all american family",
                                       "Junior cosmopolitans",
                                       "Senior cosmopolitans",
                                       "Students in apartments",
                                       "Fresh masters in the city",
                                       "Single youth",
                                       "Suburban youth",
                                       "Ethnically diverse",
                                       "Young urban have-nots",
                                       "Mixed apartment dwellers",
                                       "Young and rising", 
                                       "Young, low educated", 
                                       "Yound seniros in the city",
                                       "Own home elderly",
                                       "Seniors in apartments",
                                       "Residential elderly",
                                       "Porchless seniors: no front yard",
                                       "Religious elderly singles",
                                       "Low income catholics",
                                       "Mixed seniors2",
                                       "Lower class large families",
                                       "Large family,employed child",
                                       "Village families",
                                       "Couples with teens 'Married with children'",
                                       "Mixed small town dwellers",
                                       "Traditional families",
                                       "Large religous families",
                                       "Large family farms",
                                       "Mixed rurals"))

#Average Age Refactor
df$Avg_Age <- factor(df$Avg_Age,
                     levels=c(1:6),
                     labels=c("20-30 years",
                              "30-40 years",
                              "40-50 years",
                              "50-60 years",
                              "60-70 years",
                              "70-80 years")) 

#Custom Main Type Refactor
df$Customer_Main_Type <- factor(df$Customer_Main_Type,
                                levels=(1:10),
                                labels=c("Successful hedonists",
                                         "Driven Growers",
                                         "Average Family",
                                         "Career Loners",
                                         "Living well",
                                         "Cruising Seniors",
                                         "Retired and Religious",
                                         "Family with grown ups",
                                         "Conservatie Families",
                                         "Farmers"))

#Percentages Refactor
for (i in which(colnames(df)=="Percentage_Roman_Catholic"):which(colnames(df)=="Percentage_Purchasing_Power_Class")){
  df[,i] <- factor(df[,i],
                   levels=c(0:9),
                   labels=c("0%",
                            "1-10%",
                            "11-23%",
                            "24-36%",
                            "37-49%",
                            "50-62%",
                            "63-75%",
                            "76-88%",
                            "89-99%",
                            "100%"))
}

#Number of Refactor
for (i in which(colnames(df)=="Number_of_Contribution_Private_Third_Party_Insurance"):which(colnames(df)=="Number_of_Social_Security_Insurance_Policies")){
  df[,i] <- factor(df[,i],
                   levels=c(0:9),
                   labels=c("0",
                            "1-49",
                            "50-99",
                            "100-199",
                            "200-499",
                            "500-999",
                            "1000-4999",
                            "5000-9999",
                            "10,000-19,999",
                            ">=20,000"))
}

#Set class label as factor 
df$CARAVAN <- factor(df$CARAVAN,levels=c("0","1"))
@
I will now remove the column ORIGIN. The column origin is a factor with two values, TRAIN and TEST. It is the original set that the data came from in the challenge that this dataset was creaeted for. TRAIN data was given to contestants, and TEST was the data used to test the submitted models. As I am going to be resampling the data and partitioning my own train and test sets this column is useless so I will remove it.
<<echo=TRUE,eval=TRUE>>=
#Get rid of ORIGIN
df$ORIGIN <- NULL
@
I will now remove any rows with missing values
<<echo=TRUE,eval=TRUE>>=
#Remove NA's
df<-df[complete.cases(df),]
@
I will now resample the dataset to balance the distribution of the class label. I will do this using the ROSE package.
<<echo=TRUE,eval=TRUE>>=
library(ROSE)
@
The ROSE package (Random Over-Sampling Examples) is a package that helps deal with binary classification with imbalances classes, making it perfect for what I'm trying to do. 

I will now use the ovun.sample function from the package to oversample my dataset so that there is roughly even distribution of the class label. 
<<echo=TRUE,eval=TRUE>>=
#Resample Train(Oversampling)
df<-ovun.sample(CARAVAN~.,data=df,method="over")$data
@


\section{Modelling/Classification}
I have decided to create a random forest model for classifying my dataset. I have chosen this model because It is supposed to be a high performing classifier. It is supposed to be a robust model that can handle unbalanced data such as the dataset I have chosen. I have also already had experience with this type of model from the labs in the course. To create my models I will be using the caret and randomForest R libraries. 
<<echo=FALSE,eval=TRUE>>=
memory.limit(size=100000)
@

<<echo=TRUE,eval=TRUE>>=
library(caret)
library(randomForest)
@
The caret package (Classification and Regression Training) contains useful for splitting data that I will use to split my data into trian and test sets. Specifically I will use the function createDataParticion to create a single train and test set from all of my data that I will use to build my initial model. I will also use the function createFolds function to generate folds for 10 fold cross validation. 

The randomForest package contains functions for creating randomForest models, and for evaluating variable importance in models as well as functions to calculate and plot various accuracies and other useful information about random forest models. I will use the function randomForest to create my model, and I will the use the importance function to try and improve the accuracy of my model. I will also use the plot function from the package to plot error rates. 

First I will create a train and test set. 
<<echo=TRUE,eval=TRUE>>=
#Partition dataset using caret
part<-createDataPartition(y=df$CARAVAN,p=0.7,list=FALSE)
train<-df[part,]
test<-df[-part,]
@
I will write a function to build a random forest model, using the randomForest function from the randomForest package. I will pass the training set, test set and allow the passing of ntrees and nodesize as I plan to varie these values later. 
<<echo=TRUE,eval=TRUE>>=
#Function to build random forest model
buildModel<-function(trainData,testData,ntrees=100,nodeSize=1){
  #build random forest model
  model<-randomForest(trainData[,-ncol(trainData)],
                      trainData[,ncol(trainData)],
                      xtest=testData[,-ncol(testData)],
                      ytest=testData[,ncol(testData)],
                      ntree=ntrees,
                      nodesize=nodeSize,
                      proximity=TRUE,
                      importance=TRUE)
  #Return model
  return(model)
}
@
I will create a function to display error rates and accuracies from a model. 
<<echo=TRUE,eval=TRUE>>=
#Print Error rates and accuracies 
displayResultsFromModel<-function(model,trainRows,testRows){
  print("TRAIN")
  #Train OOB Error
  print(paste("Train OOB Error: ",
              model$err.rate[nrow(model$test$err.rate),
                                                 1,
                                                 drop=FALSE],sep=""))
  #Train Factor Level 0 Error
  print(paste("Train CARAVAN=0 Error: ",model$err.rate[nrow(model$test$err.rate),
                                                       2,
                                                       drop=FALSE],sep=""))
  #Train Factor Level 1 Error
  print(paste("Train CARAVAN=1 Error: ",model$err.rate[nrow(model$test$err.rate),
                                                       3,
                                                       drop=FALSE],sep=""))
  #Train Accuracy
  trainAuc<-sum(diag(model$confusion))/trainRows
  print(paste("Train Accuracy: ",trainAuc,sep=""))

  #Print blank line between train and test results
  print(" ")
  
  print("TEST")
  #Test Error
  print(paste("Test Error: ",model$test$err.rate[nrow(model$test$err.rate),
                                                 1,
                                                 drop=FALSE],sep=""))
  #Train Factor Level 0 Error
  print(paste("Test CARAVAN=0 Error: ",model$test$err.rate[nrow(model$test$err.rate),
                                                           2,
                                                           drop=FALSE],sep=""))
  #Train Factor Level 1 Error
  print(paste("Test CARAVAN=1 Error: ",model$test$err.rate[nrow(model$test$err.rate),
                                                           3,
                                                           drop=FALSE],sep=""))
  #Test Accuracy
  testAuc<-sum(diag(model$test$confusion))/testRows
  print(paste("Test Accuracy: ",testAuc,sep=""))
}
@
I will now use these functions to build and test my model. 
<<echo=TRUE,eval=TRUE>>=
#Build model and display accuracies 
model<-buildModel(train,test)
#Display Values
displayResultsFromModel(model,nrow(train),nrow(test))
#Plot Error Rates
plot(model)
@
Now I will take a look at the error rates and accuracies of my model. During initial testing, accuracies where around 55-57 range. The train error rate tended to be lower than the test error rate but this was expected. The error rates for the CARAVAN=1 were extremely high, near 80 percent. Error rates for CARAVAN=0 Where extremely low less than 1 percent.This isn't great and could be improved. More data where CARAVAN=TRUE is really needed. Perhaps different resampling methods like bootstraping might generate better results. 

I will write a function to validate the model. I will use a 10 fold cross validation method.
<<echo=TRUE,eval=TRUE>>=
#Function to perform 10 fold cross validation
validateModel <- function(data,ntrees=100,nodeSize=1){
  #Frame to hold results
  results<-data.frame(OOB=as.numeric(),
                      trainFalseError=as.numeric(),
                      trainTrueError=as.numeric(),
                      testError=as.numeric(),
                      testFalseError=as.numeric(),
                      testTrueError=as.numeric(),
                      trainAccuracy=as.numeric(),
                      testAccuracy=as.numeric())
  #Folds generated using Caret packages createFolds 
  folds<-createFolds(data$CARAVAN,k=10,list=TRUE,returnTrain=FALSE)
  for (i in 1:10){
    #Keep one set for testing, rest training
    trainData<-data[-c(folds[[i]]),]
    testData<-data[c(folds[[i]]),]
    model<-randomForest(trainData[,-ncol(trainData)],
                        trainData[,ncol(trainData)],
                        xtest=testData[,-ncol(testData)],
                        ytest=testData[,ncol(testData)],
                        ntree=ntrees,
                        nodesize=nodeSize,
                        proximity=TRUE)
    #TRAIN
    oob<-model$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    trainFalse<-model$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    trainTrue<-model$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    trainAccuracy<-sum(diag(model$confusion))/nrow(trainData)
    #TEST
    testError<-model$test$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    testFalse<-model$test$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    testTrue<-model$test$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    testAccuracy<-sum(diag(model$test$confusion))/nrow(testData)
    #Create new Row in results with values
    results[nrow(results)+1,]<-c(oob,
                                 trainFalse,
                                 trainTrue,
                                 testError,
                                 testFalse,
                                 testTrue,
                                 trainAccuracy,
                                 testAccuracy)
  }
  #Return results
  return(results)
}
@
I will also write a function to display the results. I will display the data frame as well as averages. 
<<echo=TRUE,eval=TRUE>>=
#Takes results and displays them as a whole and with averages 
displayResults<-function(results){
  Position=c(1:10)
  #PLOT COLUMNS
  #TRAIN
  #OOB
  plot<-ggplot(results,aes(x=Position,y=OOB)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="OOB")
  print(plot)
  #Train Caravan=0 Error
  plot<-ggplot(results,aes(x=Position,y=trainFalseError)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Train Caravan=0 Error")
  print(plot)
  #Train Caravan=1 Error
  plot<-ggplot(results,aes(x=Position,y=trainTrueError)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Train Caravan=1 Error")
  print(plot)
  #Train Accuracy
  plot<-ggplot(results,aes(x=Position,y=trainAccuracy)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Train Accuracy")
  print(plot)
  
  #TEST
  #Test Error
  plot<-ggplot(results,aes(x=Position,y=testError)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Test Error")
  print(plot)
  #Test Caravan=0 Error
  plot<-ggplot(results,aes(x=Position,y=testFalseError)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Test Caravan=0 Error")
  print(plot)
  #Test Caravan=1 Error
  plot<-ggplot(results,aes(x=Position,y=testTrueError)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Test Caravan=1 Error")
  print(plot)
  #Test Accuracy
  plot<-ggplot(results,aes(x=Position,y=testAccuracy)) 
  plot<-plot + geom_point()
  plot<-plot + geom_smooth()
  plot<-plot + labs(title="Test Accuracy")
  print(plot)
  
  #AVERAGES
  #TRAIN
  #OOB
  print(paste("Average OOB: ",
              sum(results$OOB)/nrow(results),sep=""))
  #Train CARAVAN=0 Error
  print(paste("Average CARAVAN=0 Error: ",
              sum(results$trainFalseError)/nrow(results),sep=""))
  #Train Caravan=1 Error
  print(paste("Average CARAVAN=1 Error: ",
              sum(results$trainTrueError)/nrow(results),sep=""))
  #Train Accuracy
  print(paste("Average Train Accuracy: ",
              sum(results$trainAccuracy)/nrow(results),sep=""))
  
  #Print blank line between train and test results
  print(" ")
  
  #Test Error
  print(paste("Average Test Error: ",
              sum(results$testError)/nrow(results),sep=""))
  #Test CARAVAN=0 Error
  print(paste("Average CARAVAN=0 Error: ",
              sum(results$testFalseError)/nrow(results),sep=""))
  #Test CARAVAN=1 Error
  print(paste("Average CARAVAN=1 Error: ",
              sum(results$testTrueError)/nrow(results),sep=""))
  #Test Accuracy
  print(paste("Average Test Accuracy: ",
              sum(results$testAccuracy)/nrow(results),sep=""))
}
@
I will now use these functions to validate my model
<<echo=TRUE,eval=TRUE>>=
#Validate Model
validateResult <- validateModel(df)
displayResults(validateResult)
@
Similar results to training the initial model, accuracies in the 55-57 range roughly. Although there isn't much variance in any of the values. Right now the model is almost just saying that all the rows are FALSE.Train accuracy was higher than test accuracy again. Train error rate for when CARAVAN=1 was extremely high, near 90 percent. Compared to when CARAVAN=0 which was < 1 percent. 
\section{Imporving Performance}
I will now try to improve the performance of my model 

I will start by tring to fine tune the ntrees attribute of my model by testing my model with values between 1 and 100 for ntree. I was originally going to use the range 100 to 1000 and increment by 100 trees, but during testing I found that the lowest accuracies where within the 1 too 100 range and that after 100 they only increased. I have set the increment to 10 as I don't want to overfit the model. 

I have chosen to try and fine tune this attribute as I know it has an effect on accuracy. Although I may accidently overfit the model by setting the value of ntree too low. I have found during testing that the higher the number of ntrees the greater the error rate. This is mostly due to the error rate when CARAVAN=1 increasing with the number of trees. The error rate for CARAVAN=0 tends to level out and become near linear. 

I will write a function to do this, that will return the optimal number of trees based on test accuracy.
<<echo=TRUE,eval=TRUE>>=
#Using same train and test set as before 
#Tweak number of trees 
testNTrees <- function(trainData,testData){
  ntrees<-20
  results<-NULL
  results<-data.frame(NTrees=as.numeric(),
                      OOB=as.numeric(),
                      trainFalseError=as.numeric(),
                      trainTrueError=as.numeric(),
                      testError=as.numeric(),
                      testFalseError=as.numeric(),
                      testTrueError=as.numeric(),
                      trainAccuracy=as.numeric(),
                      testAccuracy=as.numeric())
  for (i in 1:9){
    trainData=train
    testData=test
    model<-randomForest(trainData[,-ncol(trainData)],
                        trainData[,ncol(trainData)],
                        xtest=testData[,-ncol(testData)],
                        ytest=testData[,ncol(testData)],
                        ntree=ntrees,
                        proximity=TRUE)
    #TRAIN
    oob<-model$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    trainFalse<-model$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    trainTrue<-model$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    trainAccuracy<-sum(diag(model$confusion))/nrow(trainData)
    #TEST
    testError<-model$test$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    testFalse<-model$test$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    testTrue<-model$test$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    testAccuracy<-sum(diag(model$test$confusion))/nrow(testData)
    #Create new row in results with new data
    results[nrow(results)+1,]<-c(ntrees,
                                 oob,
                                 trainFalse,
                                 trainTrue,
                                 testError,
                                 testFalse,
                                 testTrue,
                                 trainAccuracy,
                                 testAccuracy)
    results
    ntrees <-ntrees + 10
  }
  #return max row
  ntrees<-results$NTrees[which.max(results$testAccuracy)]
  return(ntrees)
}
@

I will now use this function to find ntrees. 
<<echo=TRUE,eval=TRUE>>=
#Get ntrees
ntrees<-testNTrees(train,test)
ntrees
@
During testing, ntree values were not high. Usually 100 or 200 was returned 

I will not build a second model with the new values of ntrees to compare it to the original model
<<echo=TRUE,eval=TRUE>>=
#Build second model with new ntree and nodesize
model2<-buildModel(train,test,ntrees=ntrees)
#Display Values
displayResultsFromModel(model2,nrow(train),nrow(test))
#Plot Errors
plot(model2)
@

I will now validate the new model
<<echo=TRUE,eval=TRUE>>=
#Validate second model, 10 fold cross validation 
validateResults2<-validateModel(df,ntrees)
displayResults(validateResults2)
@
During testing, the changes caused a greater range in test errors and accuracies. Values now varied greatly. Train errors were increased slighly but test errors were improved slightly also. Change was roughly 1 percent in each case. Error rates for CARAVAN=1 increased also and were near 90 percent again. 

I will not try and fine tune the randomForest function variable nodesize. I will write a function to do this that works in a similar way to the function I wrote to test ntrees. Nodesize in this case refers to the minimum number of terminal nodes. By default for classification this is set to 1. This is very low and will create very large trees. I hope to increase the accuracy and decrease modeling times by varying this. 
<<echo=TRUE,eval=TRUE>>=
#Tweek Nodesize
testNodeSize <- function(trainData,testData,ntrees){
  nsize<-0
  results<-data.frame(Nodesize=as.numeric(),
                      OOB=as.numeric(),
                      trainFalseError=as.numeric(),
                      trainTrueError=as.numeric(),
                      testError=as.numeric(),
                      testFalseError=as.numeric(),
                      testTrueError=as.numeric(),
                      trainAccuracy=as.numeric(),
                      testAccuracy=as.numeric())
  for (i in 1:floor(nrow(trainData)/100)){
    model<-randomForest(trainData[,-ncol(trainData)],
                        trainData[,ncol(trainData)],
                        xtest=testData[,-ncol(testData)],
                        ytest=testData[,ncol(testData)],
                        ntree=ntrees,
                        proximity=TRUE)
    #TRAIN
    oob<-model$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    trainFalse<-model$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    trainTrue<-model$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    trainAccuracy<-sum(diag(model$confusion))/nrow(trainData)
    #TEST
    testError<-model$test$err.rate[nrow(model$test$err.rate),1,drop=FALSE]
    testFalse<-model$test$err.rate[nrow(model$test$err.rate),2,drop=FALSE]
    testTrue<-model$test$err.rate[nrow(model$test$err.rate),3,drop=FALSE]
    testAccuracy<-sum(diag(model$test$confusion))/nrow(testData)
    results[nrow(results)+1,]<-c(nsize,
                                 oob,
                                 trainFalse,
                                 trainTrue,
                                 testError,
                                 testFalse,
                                 testTrue,
                                 trainAccuracy,
                                 testAccuracy)
    nsize<-nsize+1
  }
  #Return node size 
  nodeSize<-results$Nodesize[which.max(results$testAccuracy)]
  return(nodeSize)
}
@

I will now use the function to find nodesize.
<<echo=TRUE,eval=TRUE>>=
#Get node size
nodeSize<-testNodeSize(train,test,ntrees)
nodeSize
@
During testing, the values for nodeSize where in the low 100s. Train and test error rates have improved. 

I will now test the accuracy of my new model, with the new values for ntree and nodesize.
<<echo=TRUE,eval=TRUE>>=
#Build second model with new ntree and nodesize
model3<-buildModel(train,test,ntrees=ntrees,nodeSize=nodeSize)
#Display Values
displayResultsFromModel(model3,nrow(train),nrow(test))
#Plot Results
@
I will now validate the new model
<<echo=TRUE,eval=TRUE>>=
#Validate second model, 10 fold cross validation 
validateResults3<-validateModel(df,ntrees,nodeSize)
displayResults(validateResults3)
@
Error rates and accuracies have improved. Train accuracy is not in the high 50s low 60 range. Same for test accruacy. Train accuracy is still higher than test accuracy. The biggest imporovement is in the error rate for when CARAVAN = 1. Error rates are now in mid 60s low 70s range. This still isn't great but Is a good improvement over the original model. Error rates for when CARAVAN=0 however have also increased by around 1-2 percent. This isn't a huge increase and was worth the drop in the error rates for CARAVAN=1. 

Now I will use the importance function from the randomForest package, to create plots of mean decrease in accuracy and mean decrease in Gini. I will also create a list of each ordered highest to lowest which I will use to try and remove variables which are making my model less accurate. 
<<echo=TRUE,eval=TRUE>>=
#Mean Decrease in accuracy
meanDecreaseAccuracy<-importance(model2,type=1)
#Order highest to lowest
meanDecreaseAccuracy<-meanDecreaseAccuracy[order(-meanDecreaseAccuracy),,drop=FALSE]
#Plot
varImpPlot(model2,type=1)

#Mean decrease in node impurity
meanDecreaseGini<-importance(model2,type=2)
#Order highest to lowest
meanDecreaseGini<-meanDecreaseGini[order(-meanDecreaseGini),,drop=FALSE]
#Plot
varImpPlot(model2,type=2)
@
I will remove any columns that have a negative mean decrease in accuracy value(if any). I am doing this because in theory this should remove variables which are having a negative effect on the overal accuracy of the model. 
<<echo=TRUE,eval=TRUE>>=
#Get negative or 0 MDA
cols<-rownames(meanDecreaseAccuracy[meanDecreaseAccuracy<0,,drop=FALSE])
#Show cols being removed
cols
#Remove cols
if (!is.null(cols)){
  df<-df[,!(colnames(df) %in% cols)]
  train<-train[,!(colnames(train) %in% cols)]
  test<-test[,!(colnames(test) %in% cols)]
}
@
I will now test the accuracy of the final model
<<echo=TRUE,eval=TRUE>>=
#Build final model, with removed columns based on mean decrease in accuracy
model4<-buildModel(train,test,ntrees=ntrees,nodeSize=nodeSize)
#Display values
displayResultsFromModel(model4,nrow(train),nrow(test))
#Plot Error
plot(model4)
@

I will now validate the final model
<<echo=TRUE,eval=TRUE>>=
#Validate final model, 10 fold cross validation 
validateResults4<-validateModel(df,ntrees,nodeSize)
displayResults(validateResults4)
@
During testing, only one column was removed but this did cause an increase in accuracy in both the training and testing accuracies. Error rate averages did slighly decrease, but there was a change in the range of values. Error rates for when CARAVAN=1 dropped, but error rates for CARAVAN=0 rose. In some cases by nearly 20 percent. 
\section{Conclusions}
My model was not very accurate, the real issue being the error rate in predicting when CARAVAN=1. More data would be helpful and I think a better resampling technique such as bootstrap might have helped. A different model such as a neural network might have performed better. By the final model, my model was around 60-62 percent accuracy and error rates for CARAVAN=1 were in the 60-80 percent range. 

I did originally try and remove highly correlated variables, but didn't have much success so I ommited this from the coursework. If I had more time I would retry this to see if It could improve the accuracy.

If I had more time, I would have tried to combine some the variables into new ones. I would have tried to combine the 5 social class variables for example into one variable, and classify each row as one social class. Due to time constraints I was not able to do this. 

I would have also tried to identify more important predictors and great a model with only those predictors. 

I feel like I have learned much completing this coursework despite the fact the accuracy of my model wasn't great. 
\section{References}
\begin{itemize}
  \item Wickham, H(2013). ggplot2.[online] Available at: http://ggplo2.org/ [Accessed 16th Dec 2017]  
  \item Wickham, H(2017). Package 'dplyr' v0.7.4.[PDF] Available at:https://cran.r-project.org/web/packages/dplyr/dplyr.pdf [Accessed: 16th Dec 2017]
  \item Lunardon, N(2014). Package 'ROSE' V0.0-3.[PDF] Available at:https://cran.r-project.org/web/packages/ROSE/ROSE.pdf [Accessed: 16th Dec 2017]
  \item analyticsvidhya,(2016). Practical Guide to deal with Imbalanced Classification Problems in R.[online] Available at:https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/ [Accessed: 16th Dec 2017]
  \item Liaw, A(2015). Package 'randomForest' V4.6-12. [PDF] Available at:https://cran.r-project.org/web/packages/randomForest/randomForest.pdf [Accessed: 16th Dec 2017]
  \item topepo,(2017). The caret Package.[online] Available at:http://topepo.github.io/caret/index.html [Accessed 16th Dec 2017]
\end{itemize}
\end{document}